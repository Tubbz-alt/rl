from rl_main.conf.constants_general import *
from rl_main.conf.names import *

# [GENERAL]
SEED = 1
MY_PLATFORM = OSName.MAC
PYTHON_PATH = "~/anaconda/envs/rl/bin/python"
ENV_RENDER = False

# [MQTT]
MQTT_SERVER = "localhost"
MQTT_PORT = 1883
MQTT_LOG = False

# [WORKER]
NUM_WORKERS = 1

# [OPTIMIZATION]
MAX_EPISODES = 5000
GAMMA = 0.98
#GAMMA = 1.0

# [MODE]
MODE_SYNCHRONIZATION = True
MODE_GRADIENTS_UPDATE = True      # Distributed
MODE_PARAMETERS_TRANSFER = True    # Transfer

# [TRAINING]
EPSILON_GREEDY_ACT = False
EPSILON_DECAY = True
EPSILON_START = 0.9
EPSILON_END = 0.05
EPSILON_DECAY_RATE = 1000 # Large value means low decaying
OPTIMIZER = OptimizerName.ADAM
GAE_LAMBDA = 0.99
LEARNING_RATE = 0.001

# [TRAJECTORY_SAMPLING]
TRAJECTORY_SAMPLING = True
TRAJECTORY_LIMIT_SIZE = 200
TRAJECTORY_BATCH_SIZE = 64

# [PPO]
PPO_K_EPOCH = 10
PPO_EPSILON_CLIP = 0.1
PPO_VALUE_LOSS_WEIGHT = 0.5
PPO_ENTROPY_WEIGHT = 0.01

# [1. ENVIRONMENTS]
#ENVIRONMENT_ID = EnvironmentName.BREAKOUT_DETERMINISTIC_V4
#ENVIRONMENT_ID = EnvironmentName.QUANSER_SERVO_2
#ENVIRONMENT_ID = EnvironmentName.PENDULUM_V0
#ENVIRONMENT_ID = EnvironmentName.DRONE_RACING_MAC
# ENVIRONMENT_ID = EnvironmentName.CARTPOLE_V0
ENVIRONMENT_ID = EnvironmentName.INVERTEDDOUBLEPENDULUM_V2
#ENVIRONMENT_ID = EnvironmentName.BLACKJACK_V0
#ENVIRONMENT_ID = EnvironmentName.FROZENLAKE_V0

# [2. DEEP_LEARNING_MODELS]
DEEP_LEARNING_MODEL = DeepLearningModelName.ActorCriticMLP
#DEEP_LEARNING_MODEL = DeepLearningModelName.ActorCriticCNN
#DEEP_LEARNING_MODEL = DeepLearningModelName.NoModel

# [3. ALGORITHMS]
#RL_ALGORITHM = RLAlgorithmName.DQN_V0
RL_ALGORITHM = RLAlgorithmName.PPO_V0
#RL_ALGORITHM = RLAlgorithmName.Monte_Carlo_Control_V0